{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "You do not have to follow our installation instructions if you have roughly equivalent setups / environments already.\n",
    "\n",
    "We will use Conda and Pip to help us install packages for this homework. If you do not have Miniconda or Anaconda, you can install Miniconda from here https://docs.conda.io/en/latest/miniconda.html.\n",
    "\n",
    "```\n",
    "conda create --name exercise2 python=3.7\n",
    "conda activate exercise2\n",
    "\n",
    "pip install jupyter pandas\n",
    "```\n",
    "\n",
    "Go to https://pytorch.org/ to install PyTorch if you don't have it already\n",
    "\n",
    "To install the Hugging Face `transformers` library, run\n",
    "```\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "Follow the instructions from https://docs.dgl.ai/en/0.4.x/install/ to install Deep Graph Library (DGL).\n",
    "\n",
    "Spin up jupyter notebook with\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "# Exercise\n",
    "Our exercise is an implementation of the paper [Graph-to-Tree Learning for Solving Math Word Problems](https://www.aclweb.org/anthology/2020.acl-main.362.pdf), which solves math word problems in the MAWPS dataset. Please run `demo.ipynb` for some visualizations of the overall pipeline. We recommend that you read the original paper as well if necessary.\n",
    "\n",
    "## Provided Components\n",
    "1. We provide the entire input and output processing pipeline for you, as described in `demo.ipynb`.\n",
    "2. We provide a fully implemented custom implementation of the transformer in the `TransformerBlock` class.\n",
    "3. We provide a partially implemented graph convolutional network in the `GCN` class.\n",
    "4. We provide a fully implemented tree decoding network in `TreeDecoder`. The tree decoding logic in `train` and `predict` is fully implemented.\n",
    "5. If `use_t5 = 'small'`, `setup` will load in a pretrained `t5-small` into the variable `t5_model`.\n",
    "\n",
    "## Tasks\n",
    "Your tasks are\n",
    "1. Use Deep Graph Library (DGL) to complete the graph convolution network. Keep `use_t5 = None` for this part. The baseline performance obtained by the TA is 0.74 validation value accuracy. Report validation accuracies for at least 5 sets of hyperparameters.\n",
    "2. Use Hugging Face `transformers` library to replace the custom transformer base model with a pre-trained `t5-small` model. Set `use_t5 = 'small'` for this part. The baseline performance obtained by the TA is 0.78 validation value accuracy. Report validation accuracies for at least 5 sets of hyperparameters.\n",
    "3. Change any part of the code (e.g. hyperparameter, architecture, training data, etc.) to optimize the performance of at least one of the transformers (custom or T5) to be better than the baseline performance.\n",
    "\n",
    "Note that for parts 1 and 2, you should not change any provided component at all. For part 3, you may change any part of the code.\n",
    "\n",
    "For each of these parts, please run all the cells until and including the \"Training Loop\" cell below (they are all definition cells except the actual training loop cell). When you run the training loop without completing some required part of the exercise, the code will throw a `NotImplementedError`; you should fill out the required code to fix this error, then run the training loop again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:11:36.346121Z",
     "start_time": "2020-10-31T01:11:36.339539Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import itertools\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "from util import setup, check_match, sub_nP, evaluate_prefix_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Inputs to Torch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:05:19.769631Z",
     "start_time": "2020-10-31T01:05:19.746942Z"
    }
   },
   "outputs": [],
   "source": [
    "def tensorize_data(data):\n",
    "    \"\"\"\n",
    "    Collect tensors to build the input data for the model\n",
    "    \"\"\"\n",
    "    for d in data:\n",
    "        # Indices of the in_tokens in the in_vocab\n",
    "        d['in_idxs'] = torch.tensor([in_vocab.token2idx.get(x, in_vocab.unk) for x in d['in_tokens']])\n",
    "        d['n_in'] = n_in = len(d['in_idxs'])\n",
    "        d['n_nP'] = n_nP = len(d['nP'])\n",
    "        # True if the position in the input has a quantity\n",
    "        d['nP_in_mask'] = mask = torch.zeros(n_in, dtype=torch.bool)\n",
    "        mask[d['nP_positions']] = True\n",
    "        if 'out_tokens' in d:\n",
    "            # Indices of the out_tokens in the out_vocab\n",
    "            d['out_idxs'] = torch.tensor([out_vocab.token2idx.get(x, out_vocab.unk) for x in d['out_tokens']])\n",
    "            d['n_out'] = len(d['out_idxs'])\n",
    "            # A mask where the first n_nP elements are True\n",
    "            d['nP_out_mask'] = mask = torch.zeros(n_max_nP, dtype=torch.bool)\n",
    "            mask[:n_nP] = True\n",
    "        # Graph edges for constructing the DGL graph later\n",
    "        d['qcomp_edges'] = get_quantity_comparison_edges(d)\n",
    "        d['qcell_edges'] = get_quantity_cell_edges(d)\n",
    "\n",
    "def get_quantity_comparison_edges(d):\n",
    "    \"\"\"\n",
    "    Fill out an adjacency matrix representing quantity comparisons, then convert to list of edges\n",
    "    \"\"\"\n",
    "    quants = [float(x) for x in d['nP']]\n",
    "    quant_positions = d['nP_positions']\n",
    "    assert max(quant_positions) < d['n_in']\n",
    "    adj_matrix = torch.eye(d['n_in'], dtype=np.bool)\n",
    "    for x, x_pos in zip(quants, quant_positions):\n",
    "        for y, y_pos in zip(quants, quant_positions):\n",
    "            adj_matrix[x_pos, y_pos] |= x > y\n",
    "    \"\"\"\n",
    "    Convert the adjacency matrix of the directed graph into a tuple of (src_edges, dst_edges), which\n",
    "    is the input format of dgl.graph (see https://docs.dgl.ai/generated/dgl.graph.html).\n",
    "    Hint: check out the 'nonzero' function\n",
    "    \"\"\"\n",
    "    ### Your code here ###\n",
    "    raise NotImplementedError\n",
    "\n",
    "def get_quantity_cell_edges(d):\n",
    "    \"\"\"\n",
    "    Fill out an adjacency matrix representing the quantity cell graph, then convert to list of edges\n",
    "    \"\"\"\n",
    "    in_idxs = d['in_idxs']\n",
    "    quant_positions = d['nP_positions']\n",
    "    quant_cell_positions = d['quant_cell_positions']\n",
    "    assert max(quant_cell_positions) < d['n_in']\n",
    "    word_cells = set(quant_cell_positions) - set(quant_positions)\n",
    "    adj_matrix = torch.eye(d['n_in'], dtype=torch.bool)\n",
    "    for w_pos in word_cells:\n",
    "        for q_pos in quant_positions:\n",
    "            if abs(w_pos - q_pos) < 4:\n",
    "                adj_matrix[w_pos, q_pos] = adj_matrix[q_pos, w_pos] = True\n",
    "    pos_idxs = in_idxs[quant_cell_positions]\n",
    "    for idx1, pos1 in zip(pos_idxs, quant_cell_positions):\n",
    "        for idx2, pos2 in zip(pos_idxs, quant_cell_positions):\n",
    "            if idx1 == idx2:\n",
    "                adj_matrix[pos1, pos2] = adj_matrix[pos2, pos1] = True\n",
    "    \"\"\"\n",
    "    Convert the adjacency matrix of the directed graph into a tuple of (src_edges, dst_edges), which\n",
    "    is the input format of dgl.graph (see https://docs.dgl.ai/generated/dgl.graph.html).\n",
    "    Hint: check out the 'nonzero' function\n",
    "    \"\"\"\n",
    "    ### Your code here ###\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:05:20.129946Z",
     "start_time": "2020-10-31T01:05:19.772095Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Used in Transformer Block, implements the dot-product attention\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(n_hid, n_head * (n_k * 2 + n_v))\n",
    "        self.out = nn.Linear(n_head * n_v, n_hid)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        n_batch, n_batch_max_in, n_hid = x.shape\n",
    "        q_k_v = self.qkv(x).view(n_batch, n_batch_max_in, n_head, 2 * n_k + n_v).transpose(1, 2)\n",
    "        q, k, v = q_k_v.split([n_k, n_k, n_v], dim=-1)\n",
    "\n",
    "        q = q.reshape(n_batch * n_head, n_batch_max_in, n_k)\n",
    "        k = k.reshape_as(q).transpose(1, 2)\n",
    "        qk = q.bmm(k) / np.sqrt(n_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            qk = qk.view(n_batch, n_head, n_batch_max_in, n_batch_max_in).transpose(1, 2)\n",
    "            qk[~mask] = -np.inf\n",
    "            qk = qk.transpose(1, 2).view(n_batch * n_head, n_batch_max_in, n_batch_max_in)\n",
    "        qk = qk.softmax(dim=-1)\n",
    "        v = v.reshape(n_batch * n_head, n_batch_max_in, n_v)\n",
    "        qkv = qk.bmm(v).view(n_batch, n_head, n_batch_max_in, n_v).transpose(1, 2).reshape(n_batch, n_batch_max_in, n_head * n_v)\n",
    "        out = self.out(qkv)\n",
    "        return x + out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = TransformerAttention()\n",
    "        n_inner = n_hid * 4\n",
    "        self.inner = nn.Sequential(\n",
    "            nn.Linear(n_hid, n_inner),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_inner, n_hid)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(x, mask=mask)\n",
    "        return x + self.inner(x)\n",
    "    \n",
    "class GCNBranch(nn.Module):\n",
    "    def __init__(self, n_hid_in, n_hid_out, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define a branch of the graph convolution with\n",
    "        1. GraphConv from n_hid_in to n_hid_in\n",
    "        2. ReLU\n",
    "        3. Dropout\n",
    "        4. GraphConv from n_hid_in to n_hid_out\n",
    "        \n",
    "        Note: your should call dgl.nn.GraphConv with allow_zero_in_degree=True\n",
    "        \"\"\"\n",
    "        ### Your code here ###\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, graph):\n",
    "        \"\"\"\n",
    "        Forward pass of your defined branch above\n",
    "        \"\"\"\n",
    "        ### Your code here ###\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A graph convolution network with multiple graph convolution branches\n",
    "    \"\"\"\n",
    "    def __init__(self, n_head=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleList(GCNBranch(n_hid, n_hid // n_head, dropout) for _ in range(n_head))\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_hid, n_hid),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_hid, n_hid)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(n_hid)\n",
    "\n",
    "    def forward(self, h, gt_graph, attr_graph):\n",
    "        x = h.reshape(-1, n_hid)\n",
    "        graphs = [gt_graph, gt_graph, attr_graph, attr_graph]\n",
    "        x = torch.cat([branch(x, g) for branch, g in zip(self.branches, graphs)], dim=-1).view_as(h)\n",
    "        x = h + self.layer_norm(x)\n",
    "        return x + self.feed_forward(x)\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    \"\"\"\n",
    "    Activation gate used a few times in the TreeDecoder\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Gate, self).__init__()\n",
    "        self.t = nn.Linear(n_in, n_out)\n",
    "        self.s = nn.Linear(n_in, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.t(x).tanh() * self.s(x).sigmoid()\n",
    "\n",
    "class TreeDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines parameters and methods for decoding into an expression. Used in train and predict\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        drop = nn.Dropout(dropout)\n",
    "        self.constant_embedding = nn.Parameter(torch.randn(1, out_vocab.n_constants, n_hid))\n",
    "\n",
    "        self.qp_gate = nn.Sequential(drop, Gate(n_hid, n_hid))\n",
    "        self.gts_right = nn.Sequential(drop, Gate(2 * n_hid, n_hid))\n",
    "\n",
    "        self.attn_fc = nn.Sequential(drop,\n",
    "            nn.Linear(2 * n_hid, n_hid),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hid, 1)\n",
    "        )\n",
    "        self.quant_fc = nn.Sequential(drop,\n",
    "            nn.Linear(n_hid * 3, n_hid),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hid, 1, bias=False)\n",
    "        )\n",
    "        self.op_fc = nn.Sequential(drop, nn.Linear(n_hid * 2, out_vocab.n_ops))\n",
    "\n",
    "        self.op_embedding = nn.Embedding(out_vocab.n_ops + 1, n_hid, padding_idx=out_vocab.n_ops)\n",
    "        self.gts_left = nn.Sequential(drop, Gate(n_hid * 2 + n_hid, n_hid))\n",
    "        self.gts_left_qp = nn.Sequential(drop, Gate(n_hid * 2 + n_hid, n_hid), self.qp_gate)\n",
    "\n",
    "        self.subtree_gate = nn.Sequential(drop, Gate(n_hid * 2 + n_hid, n_hid))\n",
    "\n",
    "    def gts_attention(self, q, zbar, in_mask=None):\n",
    "        \"\"\"\n",
    "        Corresponds roughly to the GTS-Attention function defined by the paper\n",
    "        \"\"\"\n",
    "        attn_score = self.attn_fc(\n",
    "            torch.cat([q.unsqueeze(1).expand_as(zbar), zbar], dim=2)\n",
    "        ).squeeze(2)\n",
    "        if in_mask is not None:\n",
    "            attn_score[~in_mask] = -np.inf\n",
    "        attn = attn_score.softmax(dim=1)\n",
    "        return (attn.unsqueeze(1) @ zbar).squeeze(1) # (n_batch, n_hid)\n",
    "\n",
    "    def gts_predict(self, qp_Gc, quant_embed, nP_out_mask=None):\n",
    "        \"\"\"\n",
    "        Corresponds roughly to the GTS-Predict functions defined by the paper\n",
    "        \"\"\"\n",
    "        quant_score = self.quant_fc(\n",
    "            torch.cat([qp_Gc.unsqueeze(1).expand(-1, quant_embed.size(1), -1), quant_embed], dim=2)\n",
    "        ).squeeze(2)\n",
    "        op_score = self.op_fc(qp_Gc)\n",
    "        pred_score = torch.cat((op_score, quant_score), dim=1)\n",
    "        if nP_out_mask is not None:\n",
    "            pred_score[:, out_vocab.base_nP:][~nP_out_mask] = -np.inf\n",
    "        return pred_score\n",
    "\n",
    "    def merge_subtree(self, op, tl, yr):\n",
    "        \"\"\"\n",
    "        Corresponds to part of the GTS-Subtree function defined by the paper\n",
    "        \"\"\"\n",
    "        return self.subtree_gate(torch.cat((op, tl, yr), dim=-1))\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Overall model containing all the neural network parameters and methods\n",
    "    1. The base seq2seq model is in self.transformer_layers if use_t5=None else self.t5_encoder\n",
    "    2. The graph convolution network is in self.gcn\n",
    "    3. The tree decoder is in self.decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        drop = nn.Dropout(dropout)\n",
    "\n",
    "        if use_t5:\n",
    "            \"\"\"\n",
    "            Use t5_model.encoder as the encoder for this model. Note that unlike the custom transformer, you don't\n",
    "            need to use an external input or positional embedding for the T5 transformer \n",
    "            (i.e. don't define self.in_embed or self.pos_emb) since it already defines them internally\n",
    "            \n",
    "            You may specify layer weights to freeze during finetuning by modifying the freeze_layers global variable\n",
    "            \"\"\"\n",
    "            ### Your code here ###\n",
    "            self.t5_encoder = None\n",
    "            raise NotImplementedError\n",
    "            \n",
    "            for i_layer, block in enumerate(self.t5_encoder.block):\n",
    "                if i_layer in freeze_layers:\n",
    "                    for param in block.parameters():\n",
    "                        param.requires_grad = False\n",
    "        else:\n",
    "            # Input embedding for custom transformer\n",
    "            self.in_embed = nn.Sequential(nn.Embedding(in_vocab.n, n_hid, padding_idx=in_vocab.pad), drop)\n",
    "            # Positional embedding for custom transformer\n",
    "            self.pos_embed = nn.Embedding(1 + n_max_in, n_hid) # Use the first position as global vector\n",
    "            self.transformer_layers = nn.ModuleList(TransformerBlock() for _ in range(n_layers))\n",
    "\n",
    "        self.gcn = GCN()\n",
    "\n",
    "        self.decoder = TreeDecoder()\n",
    "\n",
    "        if not use_t5:\n",
    "            self.apply(self.init_weight)\n",
    "\n",
    "    def init_weight(self, m):\n",
    "        if type(m) in [nn.Embedding]:\n",
    "            nn.init.normal_(m.weight, 0, 0.1)\n",
    "\n",
    "    def encode(self, in_idxs, n_in, gt_graph, attr_graph, in_mask=None):\n",
    "        in_idxs_pad = F.pad(in_idxs, (1, 0), value=in_vocab.pad)\n",
    "        if use_t5:\n",
    "            \"\"\"\n",
    "            Use your T5 encoder to encoder the input indices. Note that you do NOT need to use an input embedding or\n",
    "            positional embedding (e.g. self.in_embed or self.pos_embed) for T5, since it already defines\n",
    "            the embeddings internally\n",
    "            \"\"\"\n",
    "            ### Your code here ###\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            x = self.in_embed(in_idxs_pad) # (n_batch, n_batch_max_in, n_hid)\n",
    "            h = x + self.pos_embed(torch.arange(x.size(1), device=x.device))\n",
    "            for layer in self.transformer_layers:\n",
    "                h = layer(h, mask=in_mask)\n",
    "        zg, h = h[:, 0], h[:, 1:]\n",
    "        zbar = self.gcn(h, gt_graph, attr_graph)\n",
    "        return zbar, zg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:05:20.470291Z",
     "start_time": "2020-10-31T01:05:20.134317Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Node for tree traversal during training\n",
    "    \"\"\"\n",
    "    def __init__(self, up):\n",
    "        self.up = up\n",
    "        self.is_root = up is None\n",
    "        self.left = self.right = None\n",
    "        self.ql = self.tl = self.op = None\n",
    "\n",
    "def train(batch, model, opt):\n",
    "    \"\"\"\n",
    "    Compute the loss on a batch of inputs, and take a step with the optimizer\n",
    "    \"\"\"\n",
    "    n_batch = len(batch)\n",
    "\n",
    "    n_in = [d['n_in'] for d in batch]\n",
    "    pad = lambda x, value: nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=value)\n",
    "    in_idxs = pad([d['in_idxs'] for d in batch], in_vocab.pad).to(device)\n",
    "    in_mask = pad([torch.ones(n, dtype=torch.bool) for n in n_in], False).to(device)\n",
    "    nP_in_mask = pad([d['nP_in_mask'] for d in batch], False).to(device)\n",
    "    nP_out_mask = torch.stack([d['nP_out_mask'] for d in batch]).to(device)\n",
    "    \n",
    "    qcomp_graph, qcell_graph = [], []\n",
    "    for d in batch:\n",
    "        \"\"\"\n",
    "        Create qcomp_graph and qcell_graph from d['qcomp_edges'] and d['qcell_edges'] by calling dgl.graph\n",
    "        (see https://docs.dgl.ai/generated/dgl.graph.html)\n",
    "\n",
    "        Note that num_nodes needs to be set to the maximum input length in this batch\n",
    "        \"\"\"\n",
    "        ### Your code here ###\n",
    "        qcomp_graph_i = None\n",
    "        qcell_graph_i = None\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        qcomp_graph.append(qcomp_graph_i)\n",
    "        qcell_graph.append(qcell_graph_i)\n",
    "    qcomp_graph = dgl.batch(qcomp_graph)\n",
    "    qcell_graph = dgl.batch(qcell_graph)\n",
    "    \n",
    "    label = pad([d['out_idxs'] for d in batch], out_vocab.pad)\n",
    "    nP_candidates = [d['nP_candidates'] for d in batch]\n",
    "\n",
    "    zbar, qroot = model.encode(in_idxs, n_in, qcomp_graph, qcell_graph, in_mask=None)\n",
    "    z_nP = zbar.new_zeros((n_batch, n_max_nP, n_hid))\n",
    "    z_nP[nP_out_mask] = zbar[nP_in_mask]\n",
    "\n",
    "    decoder = model.decoder\n",
    "\n",
    "    n_quant = out_vocab.n_constants + n_max_nP\n",
    "    quant_embed = torch.cat([decoder.constant_embedding.expand(n_batch, -1, -1), z_nP], dim=1) # (n_batch, n_quant, n_hid)\n",
    "\n",
    "    nodes = np.array([Node(None) for _ in range(n_batch)])\n",
    "    op_min, op_max = out_vocab.base_op, out_vocab.base_op + out_vocab.n_ops\n",
    "    quant_min, quant_max = out_vocab.base_quant, out_vocab.base_quant + n_quant\n",
    "\n",
    "    # Initialize root node vector according to zg (the global context)\n",
    "    qp = decoder.qp_gate(qroot)\n",
    "    scores = []\n",
    "    for i, label_i in enumerate(label.T): # Iterate over the output positions\n",
    "        Gc = decoder.gts_attention(qp, zbar, in_mask)\n",
    "        qp_Gc = torch.cat([qp, Gc], dim=1) # (n_batch, 2 * n_hid)\n",
    "\n",
    "        score = decoder.gts_predict(qp_Gc, quant_embed, nP_out_mask)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Whether the label is an operator\n",
    "        is_op = (op_min <= label_i) & (label_i < op_max)\n",
    "        # Whether the label is a quantity\n",
    "        is_quant = ((quant_min <= label_i) & (label_i < quant_max)) | (label_i == out_vocab.unk)\n",
    "\n",
    "        op_embed = decoder.op_embedding((label_i[is_op] - out_vocab.base_op).to(device))\n",
    "        qp_Gc_op = torch.cat([qp_Gc[is_op], op_embed], dim=1)\n",
    "\n",
    "        is_left = np.zeros(n_batch, dtype=np.bool)\n",
    "        qleft_qp = decoder.gts_left_qp(qp_Gc_op)\n",
    "        qleft = decoder.gts_left(qp_Gc_op)\n",
    "        for j, ql, op in zip(is_op.nonzero(as_tuple=True)[0], qleft, op_embed):\n",
    "            node = nodes[j]\n",
    "            nodes[j] = node.left = Node(node)\n",
    "            node.op = op\n",
    "            node.ql = ql\n",
    "            is_left[j] = True\n",
    "\n",
    "        is_right = np.zeros(n_batch, dtype=np.bool)\n",
    "        nP_score = score[:, out_vocab.base_nP:].detach().cpu()\n",
    "        ql_tl = []\n",
    "        for j in is_quant.nonzero(as_tuple=True)[0]:\n",
    "            if label_i[j] == out_vocab.unk:\n",
    "                candidates = nP_candidates[j][i]\n",
    "                label_i[j] = out_vocab.base_nP + candidates[nP_score[j, candidates].argmax()]\n",
    "\n",
    "            node = nodes[j]\n",
    "            pnode = node.up\n",
    "            t = quant_embed[j, label_i[j] - out_vocab.base_quant]\n",
    "            while pnode and pnode.right is node:\n",
    "                t = decoder.merge_subtree(pnode.op, pnode.tl, t) # merge operator, left subtree, and right child\n",
    "                node, pnode = pnode, pnode.up # backtrack to parent node\n",
    "            if pnode is None: # Finished traversing tree of j\n",
    "                continue\n",
    "            # Now pnode.left is node. t is the tl representing the left subtree of pnode\n",
    "            pnode.tl = t\n",
    "            ql_tl.append(torch.cat([pnode.ql, pnode.tl])) # For computing qright\n",
    "            nodes[j] = pnode.right = Node(pnode)\n",
    "            is_right[j] = True\n",
    "\n",
    "        qp = torch.zeros((n_batch, n_hid), device=device)\n",
    "        qp[is_left] = qleft_qp\n",
    "        if ql_tl:\n",
    "            qp[is_right] = decoder.gts_right(torch.stack(ql_tl))\n",
    "\n",
    "    label = label.to(device).view(-1)\n",
    "    scores = torch.stack(scores, dim=1).view(-1, out_vocab.n_ops + n_quant)\n",
    "    loss = F.cross_entropy(scores, label, ignore_index=out_vocab.pad)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction (for Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:05:20.735118Z",
     "start_time": "2020-10-31T01:05:20.473699Z"
    }
   },
   "outputs": [],
   "source": [
    "class BeamNode(Node):\n",
    "    \"\"\"\n",
    "    Node for beam search during evaluation\n",
    "    \"\"\"\n",
    "    def __init__(self, up, prev, qp, token=None):\n",
    "        super().__init__(up)\n",
    "        self.prev = prev\n",
    "        self.qp = qp\n",
    "        self.token = token\n",
    "\n",
    "    def trace_tokens(self, *last_token):\n",
    "        if self.prev is None:\n",
    "            return list(last_token)\n",
    "        tokens = self.prev.trace_tokens()\n",
    "        tokens.append(self.token)\n",
    "        tokens.extend(last_token)\n",
    "        return tokens\n",
    "\n",
    "def predict(d, model, beam_size=5, n_max_out=45):\n",
    "    \"\"\"\n",
    "    Predict the idxs corresponding to an expression given the inputs. Leverages beam search to maximize\n",
    "    prediction probability\n",
    "    \"\"\"\n",
    "    in_idxs = d['in_idxs'].unsqueeze(0).to(device=device)\n",
    "    \"\"\"\n",
    "    Create qcomp_graph and qcell_graph from d['qcomp_edges'] and d['qcell_edges'] by calling dgl.graph\n",
    "    (see https://docs.dgl.ai/generated/dgl.graph.html)\n",
    "    \"\"\"\n",
    "    ### Your code here ###\n",
    "    qcomp_graph = None\n",
    "    qcell_graph = None\n",
    "    raise NotImplementedError\n",
    "\n",
    "    zbar, qroot = model.encode(in_idxs, [d['n_in']], qcomp_graph, qcell_graph)\n",
    "    z_nP = zbar[:, d['nP_positions']]\n",
    "\n",
    "    decoder = model.decoder\n",
    "\n",
    "    quant_embed = torch.cat([decoder.constant_embedding, z_nP], dim=1) # (1, n_quant, n_hid)\n",
    "    op_min, op_max = out_vocab.base_op, out_vocab.base_op + out_vocab.n_ops\n",
    "\n",
    "    best_done_beam = (-np.inf, None, None)\n",
    "    beams = [(0, BeamNode(up=None, prev=None, qp=decoder.qp_gate(qroot)))]\n",
    "    for _ in range(n_max_out):\n",
    "        new_beams = []\n",
    "        for logp_prev, node in beams:\n",
    "            Gc = decoder.gts_attention(node.qp, zbar)\n",
    "            qp_Gc = torch.cat([node.qp, Gc], dim=1) # (2 * n_hid,)\n",
    "\n",
    "            log_prob = decoder.gts_predict(qp_Gc, quant_embed).log_softmax(dim=1)\n",
    "            top_logps, top_tokens = log_prob.topk(beam_size, dim=1)\n",
    "            for logp_token_, out_token_ in zip(top_logps.unbind(dim=1), top_tokens.unbind(dim=1)):\n",
    "                out_token = out_token_.item()\n",
    "                logp = logp_prev + logp_token_.item()\n",
    "                if op_min <= out_token < op_max:\n",
    "                    op_embed = decoder.op_embedding(out_token_)\n",
    "                    qp_Gc_op = torch.cat([qp_Gc, op_embed], dim=1)\n",
    "                    prev_node = copy(node)\n",
    "                    next_node = prev_node.left = BeamNode(\n",
    "                        up=prev_node, prev=prev_node,\n",
    "                        qp=decoder.gts_left_qp(qp_Gc_op),\n",
    "                        token=out_token\n",
    "                    )\n",
    "                    prev_node.op = op_embed\n",
    "                    prev_node.ql = decoder.gts_left(qp_Gc_op)\n",
    "                else:\n",
    "                    pnode, prev_node = node.up, node\n",
    "                    t = quant_embed[:, out_token - out_vocab.base_quant]\n",
    "                    while pnode and pnode.tl is not None:\n",
    "                        t = decoder.merge_subtree(pnode.op, pnode.tl, t)\n",
    "                        node, pnode = pnode, pnode.up\n",
    "                    if pnode is None:\n",
    "                        best_done_beam = max(best_done_beam, (logp, prev_node, out_token))\n",
    "                        continue\n",
    "                    pnode = copy(pnode)\n",
    "                    pnode.tl = t\n",
    "                    next_node = pnode.right = BeamNode(\n",
    "                        up=pnode, prev=prev_node,\n",
    "                        qp=decoder.gts_right(torch.cat([pnode.ql, pnode.tl], dim=1)),\n",
    "                        token=out_token\n",
    "                    )\n",
    "                new_beams.append((logp, next_node))\n",
    "        beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_size]\n",
    "        done_logp, done_node, done_last_token = best_done_beam\n",
    "        if not len(beams) or done_logp >= beams[0][0]:\n",
    "            break\n",
    "    return done_node.trace_tokens(done_last_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "We provide the training loop below. When you change the hyperparameters, make sure you keep track of which hyperparameters you were using, because you'll need those parameters again during prediction (see next section). Note that if you make multiple runs with the same `use_t5` value, the saved models will be overwritten, so make sure to copy your the `model_save_dir` somewhere else if you want to save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:01:49.300495Z",
     "start_time": "2020-10-31T00:37:45.465278Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "use_t5 = None # Value should be None, 'small', or 'base'\n",
    "model_save_dir = f'models/{use_t5 or \"custom\"}'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# IMPORTANT NOTE: if you change some of these hyperparameters during training,\n",
    "# you will also need to change them during prediction (see next section)\n",
    "n_max_in = 100\n",
    "n_epochs = 100\n",
    "n_batch = 64\n",
    "learning_rate = 1e-3\n",
    "if use_t5:\n",
    "    # T5 hyperparameters\n",
    "    freeze_layers = []\n",
    "    weight_decay = 1e-5\n",
    "    n_hid = dict(small=512, base=768)[use_t5] # Do not modify unless you want to try t5-large\n",
    "else:\n",
    "    # Custom transformer hyperparameters\n",
    "    n_layers = 3\n",
    "    n_hid = 512\n",
    "    n_k = n_v = 64\n",
    "    n_head = 8\n",
    "    weight_decay = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "train_data, val_data, in_vocab, out_vocab, n_max_nP, t5_model = setup(use_t5)\n",
    "tensorize_data(itertools.chain(train_data, val_data))\n",
    "\n",
    "model = Model()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, n_epochs)\n",
    "model.to(device)\n",
    "\n",
    "epoch = 0\n",
    "while epoch < n_epochs:\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for start in trange(0, len(train_data), n_batch):\n",
    "        batch = sorted(train_data[start: start + n_batch], key=lambda d: -d['n_in'])\n",
    "        loss = train(batch, model, opt)\n",
    "        losses.append(loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Training loss: {np.mean(losses):.3g}')\n",
    "\n",
    "    epoch += 1\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        value_match, equation_match = [], []\n",
    "        with torch.no_grad():\n",
    "            for d in tqdm(val_data):\n",
    "                if d['is_quadratic']: # This method is not equiped to handle equations with quadratics\n",
    "                    val_match = eq_match = False\n",
    "                else:\n",
    "                    pred = predict(d, model)\n",
    "                    d['pred_tokens'] = [out_vocab.idx2token[idx] for idx in pred]\n",
    "                    val_match, eq_match = check_match(pred, d)\n",
    "                value_match.append(val_match)\n",
    "                equation_match.append(eq_match)\n",
    "        print(f'Validation expression accuracy: {np.mean(equation_match):.3g}')\n",
    "        print(f'Validation value accuracy: {np.mean(value_match):.3g}')\n",
    "        # We save the model every 10 epochs, feel free to load in a trained model with\n",
    "        # model.load_state_dict(torch.load(f'models/model-{epoch}.pth'))\n",
    "        # Note: if you want to restart training from a saved model, you must also save and load the optimizer with\n",
    "        # torch.save(opt.state_dict(), os.path.join(model_save_dir, f'opt-{epoch}.pth'))\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, f'model-{epoch}.pth'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Once you have trained your model to a satisfactory accuracy, you can load in a checkpoint to predict on the test set. The output is a `'predictions.csv'` file in your directory, and you should submit this directly to the [Kaggle server](https://www.kaggle.com/t/7bf8b542b96f4214b0cca1e4d9b0bb17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:13:09.747362Z",
     "start_time": "2020-10-31T01:11:41.063125Z"
    }
   },
   "outputs": [],
   "source": [
    "use_t5 = 'small'\n",
    "eval_epoch = 30\n",
    "device = 'cpu'\n",
    "\n",
    "# Make sure your parameter here is the exact same as the parameters you trained with,\n",
    "# else the model will not load correctly\n",
    "n_max_in = 100\n",
    "if use_t5:\n",
    "    # T5 hyperparameters\n",
    "    freeze_layers = []\n",
    "    n_hid = dict(small=512, base=768)[use_t5] # Do not modify unless you want to try t5-large\n",
    "else:\n",
    "    # Custom transformer hyperparameters\n",
    "    n_layers = 3\n",
    "    n_hid = 512\n",
    "    n_k = n_v = 64\n",
    "    n_head = 8\n",
    "\n",
    "test_data, in_vocab, out_vocab, n_max_nP, t5_model = setup(use_t5, do_eval=True)\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(f'models/{use_t5 or \"custom\"}/model-{eval_epoch}.pth'))\n",
    "tensorize_data(test_data)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for d in tqdm(test_data): # There's no quadratics in the test_data, fortunately\n",
    "        pred = predict(d, model)\n",
    "        d['pred_tokens'] = pred_tokens = [out_vocab.idx2token[idx] for idx in pred]\n",
    "        d['subbed_tokens'] = subbed_tokens = sub_nP(pred_tokens, d['nP'])\n",
    "        d['Predicted'] = round(evaluate_prefix_expression(subbed_tokens), 3) # Make sure to round to 3 decimals\n",
    "\n",
    "import pandas as pd\n",
    "predictions = pd.DataFrame(test_data).set_index('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:14:25.376654Z",
     "start_time": "2020-10-31T01:14:25.329127Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions[['pred_tokens', 'subbed_tokens', 'Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T02:18:39.234190Z",
     "start_time": "2020-10-31T02:18:39.193908Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions[['Predicted']].replace([np.inf, -np.inf, np.nan], 0).to_csv('prediction.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
